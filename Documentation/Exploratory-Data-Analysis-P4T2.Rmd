---
title: "Exploratory Data Analysis for Seattle Building Energy Analysis"
author: "P4T2"
date: "2024-03-05"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(sf)
library(readr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(spdep)
library(leaflet)
library(RColorBrewer)
```

# Introduction

Briefly describe the project, objectives, and data sources. Test, Connor working w git.

```{r read data, echo=FALSE}

# Read the PSE and Equity shape files for GIS
pse_data <- st_read("../Data/PSE_Gas_Tracts_2.shp")
equity_data <- st_read("../Data/Race_and_Social_Equity_Composite_Index_for_2020_Census_Tract_Geographies.shp")


#Read in building performance data
library(dplyr)
data2022 <- read.csv("../Data/Performance_Ranges_by_Building_Type_2022.csv")
data2021 <- read.csv("../Data/Performance_Ranges_by_Building_Type_2021.csv")
data2020 <- read.csv("../Data/Performance_Ranges_by_Building_Type_2020.csv")
data2019 <- read.csv("../Data/Performance_Ranges_By_Building_Type_2019.csv")
data2018 <- read.csv("../Data/Performance_Ranges_By_Building_Type_2018.csv")
data2017 <- read.csv("../Data/Performance_Ranges_by_Building_Type_2017.csv")
data2016 <- read.csv("../Data/Performance_Ranges_By_Building_Type_2016.csv")

```

# Data Cleaning

```{r Identifying Missing Values, echo=FALSE}

# Summarise each column to count the number of NA values - PSE Data
missing_summary_PSE <- pse_data %>%
  st_set_geometry(NULL) %>% 
  summarise(across(everything(), ~sum(is.na(.)))) %>% 
  gather(key = "column", value = "missing_values")
print(missing_summary_PSE)

# Summarise each column to count the number of NA values - Equity Data
missing_summary_Equity <- equity_data %>%
  st_set_geometry(NULL) %>% 
  summarise(across(everything(), ~sum(is.na(.)))) %>% 
  gather(key = "column", value = "missing_values")
print(missing_summary_Equity)



```

Discuss your strategy for identifying and handling missing values.

There are no missing values in our equity data, but PSE data does have missing values. Reading the description of the PSE data, the missing atributes is due to 'grouped flag' not being filtered to true. This flag indicates data aggregation across multiple census tracts to preserve privacy, reflecting broader trends without risking the identification of individual tracts. Entries with the 'grouped flag' set to false may have values of 0, signifying no or insufficient data for aggregation. 


```{r Handling Missing Values, echo=FALSE}

# Filter the PSE data for the Grouped_Flag to TRUE
pse_data_clean <- subset(pse_data, Grouped_Fl=='TRUE')

# Summarise each column to count the number of NA values - PSE Data
missing_summary_PSE_clean <- pse_data_clean %>%
  st_set_geometry(NULL) %>% 
  summarise(across(everything(), ~sum(is.na(.)))) %>% 
  gather(key = "column", value = "missing_values")
print(missing_summary_PSE_clean)

#Building Performance Data
#For 2017 specifically there are 2 columns that were giving us issues for data cleaning immediately.

#Other than 2017 we are just omitting files that have na values
data2017 <- select(data2017, -c(Low.Outlier.Cutoff, High.Outlier.Cutoff))
data2022 <- na.omit(data2022)
data2021 <- na.omit(data2021)
data2020 <- na.omit(data2020)
data2019 <- na.omit(data2019)
data2018 <- na.omit(data2018)
data2017 <- na.omit(data2017)
data2016 <- na.omit(data2016)

```

We no longer have any missing values in our PSE data.
No more NA/missing values in Building Performance Data

```{r Converting and Manipulating Character Data, echo=FALSE}

# Ensure the GEOID and Census_Tract columns are of the same type before merging
pse_data_clean$Census_Tra <- as.character(pse_data_clean$Census_Tra)
equity_data$GEOID <- as.character(equity_data$GEOID)

# Convert to regular dataframes for the merge operation, and drop geometry column from equity_data
pse_data_df <- as.data.frame(pse_data_clean)
equity_data_df <- as.data.frame(equity_data %>% st_set_geometry(NULL))

# Renaming Columns for Clarity in pse_data_df
pse_data_df <- pse_data_df %>% 
  rename(
    Grouped_Flag = Grouped_Fl, 
    Usage_MMBTU = Usage_MMBT, 
    Number_Accts = Number_Acc, 
    Emissions_MTCO2E = Emissions_, 
    USAGEPERCAPITA_MMBTU = USAGEPERCA, 
    EMISSIONSPERCAPITA_MTCO2E = EMISSIONSP, 
    RSE_Quintile = RSE_Quinti, 
    SHAPE_Length = SHAPE_Leng
  )

# Standardizing Quintile Labels in pse_data_df
pse_data_df$RSE_Quintile <- replace(pse_data_df$RSE_Quintile, pse_data_df$RSE_Quintile == 'Highest priority/Most disadvantaged', 'Highest')


# Creating a Subset of Relevant Columns in equity_data_df
equity_data_df <- subset(equity_data_df, select = c(OBJECTID, GEOID, SHAPE_Leng, SHAPE_Area, RACE_ELL_1, SOCIOECO_2, HEALTH_D_1, COMPOSIT_1))

# Renaming Columns for Clarity in equity_data_df
equity_data_df <- equity_data_df %>% 
  rename(
    SHAPE_Length = SHAPE_Leng, 
    RACE_ELL_ORIGINS_QUINTILE = RACE_ELL_1, 
    SOCIOECON_DISADV_QUINTILE = SOCIOECO_2, 
    HEALTH_DISADV_QUINTILE = HEALTH_D_1, 
    COMPOSITE_QUINTILE = COMPOSIT_1
  )


# Standardizing Quintile Labels in equity_data_df
equity_data_df[equity_data_df == 'Highest Equity Priority' | equity_data_df == 'Highest Equity Priority/Most Disadvantaged' | equity_data_df == 'Highest priority/Most disadvantaged'] <- 'Highest'

```



```{r Ensuring Data Integrity, echo=FALSE}

# Merging the PSE data with the equity data on GEOID/Census_Tract
merged_data_df <- inner_join(pse_data_df, equity_data_df, by = c("Census_Tra" = "GEOID"))

# Reattach the geometry to merged_data_df
geometry_col <- pse_data_clean %>% 
  select(Census_Tra, geometry) %>% 
  distinct(Census_Tra, .keep_all = TRUE)

# Join this geometry data back to the merged dataframe
GIS_EE <- merged_data_df %>%
  left_join(geometry_col, by = "Census_Tra") %>%
  st_as_sf(crs = st_crs(pse_data_clean))


#Building Performance Data

#We are renaming all the columns names for all the years so they are all uniform so we can join them later. The order does not matter right now, we are going with the order of how they are in their respective files.

colnames(data2016) <- c("BuildingType", "Percentile_25th_EUI", "Median_EUI", "Percentile_75th_EUI", "Median_EUI_WN", "Median_Source_EUI", "Median_ES_Score", "Number_of_Buildings", "Total_GFA", "Median_GFA", "Median_Year_Built", "Percent_Electricity", "Percent_Gas", "Percent_Steam", "Percent_Other_Fuel")

colnames(data2017) <- c("BuildingType", "Percentile_25th_EUI", "Median_EUI", "Percentile_75th_EUI", "Median_EUI_WN", "Median_Source_EUI", "Median_ES_Score", "Number_of_Buildings", "Number_of_Buildings_with_ES_Score", "Total_GFA", "Median_GFA", "Median_Year_Built", "Percent_Electricity", "Percent_Gas", "Percent_Steam", "Percent_Other_Fuel")

colnames(data2018) <- c("BuildingType", "Number_of_Buildings", "Percentile_25th_EUI", "Median_EUI", "Percentile_75th_EUI", "Average_Site_EUI", "Average_Site_EUI_WN", "Median_EUI", "Median_ES_Score", "Total_GFA", "Median_GFA", "Median_Year_Built", "Not applies")

colnames(data2019) <- c("BuildingType", "Number_of_Buildings", "Percentile_25th_EUI", "Median_EUI", "Percentile_75th_EUI", "Median_EUI_WN", "Average_Site_EUI", "Average_Site_EUI_WN", "Median_Source_EUI", "Median_ES_Score", "Average_ES_Score", "Total_GFA", "Median_GFA", "Median_Year_Built", "Average_GHGSF", "MedianGHGSF")

colnames(data2020) <- c("BuildingType", "Number_of_Buildings", "Percentile_25th_EUI", "Median_EUI", "Percentile_75th_EUI", "Median_EUI_WN", "Average_Site_EUI", "Average_Site_EUI_WN", "Median_Source_EUI", "Median_ES_Score", "Average_ES_Score", "Total_GFA", "Median_GFA", "Median_Year_Built", "Average_GHGSF", "MedianGHGSF")

colnames(data2021) <- c("BuildingType", "Number_of_Buildings", "Percentile_25th_EUI", "Median_EUI", "Percentile_75th_EUI", "Median_EUI_WN", "Average_Site_EUI", "Average_Site_EUI_WN", "Median_Source_EUI", "Median_ES_Score", "Average_ES_Score", "Total_GFA", "Median_GFA", "Median_Year_Built", "Average_GHGSF", "MedianGHGSF")

colnames(data2022) <- c("BuildingType", "Number_of_Buildings", "Percentile_25th_EUI", "Median_EUI", "Percentile_75th_EUI", "Median_EUI_WN", "Average_Site_EUI", "Average_Site_EUI_WN", "Median_Source_EUI", "Median_ES_Score", "Average_ES_Score", "Total_GFA", "Median_GFA", "Median_Year_Built", "Average_GHGSF", "MedianGHGSF")

#we are doign 3 checks to see if every column in every year is in 2022, 2016 and 2018 respecitvely as I was having issues with incompatible dataset before
data2016 <- data2016[, colnames(data2016) %in% colnames(data2022)]
data2017 <- data2017[, colnames(data2017) %in% colnames(data2022)]
data2018 <- data2018[, colnames(data2018) %in% colnames(data2022)]
data2019 <- data2019[, colnames(data2019) %in% colnames(data2022)]
data2020 <- data2020[, colnames(data2020) %in% colnames(data2022)]
data2021 <- data2021[, colnames(data2021) %in% colnames(data2022)]

data2016 <- data2016[, colnames(data2016) %in% colnames(data2016)]
data2017 <- data2017[, colnames(data2017) %in% colnames(data2016)]
data2018 <- data2018[, colnames(data2018) %in% colnames(data2016)]
data2019 <- data2019[, colnames(data2019) %in% colnames(data2016)]
data2020 <- data2020[, colnames(data2020) %in% colnames(data2016)]
data2021 <- data2021[, colnames(data2021) %in% colnames(data2016)]
data2022 <- data2022[, colnames(data2022) %in% colnames(data2016)]

data2016 <- data2016[, colnames(data2016) %in% colnames(data2018)]
data2017 <- data2017[, colnames(data2017) %in% colnames(data2018)]
data2018 <- data2018[, colnames(data2018) %in% colnames(data2018)]
data2019 <- data2019[, colnames(data2019) %in% colnames(data2018)]
data2020 <- data2020[, colnames(data2020) %in% colnames(data2018)]
data2021 <- data2021[, colnames(data2021) %in% colnames(data2018)]
data2022 <- data2022[, colnames(data2022) %in% colnames(data2018)]

#we are adding a year column to every dataset 
library(dplyr)
data2022 <- mutate(data2022, Year = 2022)
data2021 <- mutate(data2021, Year = 2021)
data2020 <- mutate(data2020, Year = 2020)
data2019 <- mutate(data2019, Year = 2019)
data2018 <- mutate(data2018, Year = 2018)
data2017 <- mutate(data2017, Year = 2017)
data2016 <- mutate(data2016, Year = 2016)

#we are now going to combine all our datasets
combined_data <- bind_rows(
  mutate(data2016, Year = 2016),
  mutate(data2017, Year = 2017),
  mutate(data2018, Year = 2018),
  mutate(data2019, Year = 2019),
  mutate(data2020, Year = 2020),
  mutate(data2021, Year = 2021),
  mutate(data2022, Year = 2022)
)

```



```{r Special Coding for Numerical Data (Binning, etc.), echo=FALSE}

# Encoding quantiles into numeric values for GIS_EE_encoded
GIS_EE_encoded <- GIS_EE

# Encode quintile values into numeric
GIS_EE_encoded$COMPOSITE_QUINTILE <- ifelse(GIS_EE_encoded$COMPOSITE_QUINTILE == "Highest", 5, ifelse(GIS_EE_encoded$COMPOSITE_QUINTILE == "Second Highest", 4, ifelse(GIS_EE_encoded$COMPOSITE_QUINTILE == "Middle", 3, ifelse(GIS_EE_encoded$COMPOSITE_QUINTILE == "Second Lowest", 2, ifelse(GIS_EE_encoded$COMPOSITE_QUINTILE == "Lowest", 1, GIS_EE_encoded$COMPOSITE_QUINTILE)))))

GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE <- ifelse(GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE == "Highest", 5, ifelse(GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE == "Second Highest", 4, ifelse(GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE == "Middle", 3, ifelse(GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE == "Second Lowest", 2, ifelse(GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE == "Lowest", 1, GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE)))))

GIS_EE_encoded$HEALTH_DISADV_QUINTILE <- ifelse(GIS_EE_encoded$HEALTH_DISADV_QUINTILE == "Highest", 5, ifelse(GIS_EE_encoded$HEALTH_DISADV_QUINTILE == "Second Highest", 4, ifelse(GIS_EE_encoded$HEALTH_DISADV_QUINTILE == "Middle", 3, ifelse(GIS_EE_encoded$HEALTH_DISADV_QUINTILE == "Second Lowest", 2, ifelse(GIS_EE_encoded$HEALTH_DISADV_QUINTILE == "Lowest", 1, GIS_EE_encoded$HEALTH_DISADV_QUINTILE)))))

GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE <- ifelse(GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE == "Highest", 5, ifelse(GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE == "Second Highest", 4, ifelse(GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE == "Middle", 3, ifelse(GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE == "Second Lowest", 2, ifelse(GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE == "Lowest", 1, GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE)))))

# Convert encoded quintiles to numeric
GIS_EE_encoded$COMPOSITE_QUINTILE <- as.numeric(GIS_EE_encoded$COMPOSITE_QUINTILE)
GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE <- as.numeric(GIS_EE_encoded$RACE_ELL_ORIGINS_QUINTILE)
GIS_EE_encoded$HEALTH_DISADV_QUINTILE <- as.numeric(GIS_EE_encoded$HEALTH_DISADV_QUINTILE)
GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE <- as.numeric(GIS_EE_encoded$SOCIOECON_DISADV_QUINTILE)

```


```{r Identifying Outliers}

# Function to identify and summarize outliers in a column
summarize_outliers <- function(data, column_name) {
  attribute_data <- data[[column_name]]
  
  # Calculate IQR
  Q1 <- quantile(attribute_data, 0.25, na.rm = TRUE)
  Q3 <- quantile(attribute_data, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Identify outliers
  outliers_logical <- attribute_data < (Q1 - 1.5 * IQR) | attribute_data > (Q3 + 1.5 * IQR)
  outlier_values <- attribute_data[outliers_logical]

  # Calculating the minimum and maximum outlier values
  min_outlier_value <- if(length(outlier_values) > 0) min(outlier_values, na.rm = TRUE) else NA
  max_outlier_value <- if(length(outlier_values) > 0) max(outlier_values, na.rm = TRUE) else NA
  
  # Basic statistics
  basic_stats <- summary(attribute_data)
  
  # Summarize outliers
  num_outliers <- sum(outliers_logical, na.rm = TRUE)
  
  return(list(
    column = column_name,
    num_outliers = num_outliers,
    min_outlier_value = min_outlier_value,
    max_outlier_value = max_outlier_value,
    basic_stats = basic_stats
  ))
}

# Columns to check for outliers and summarize
columns_to_check <- c("EMISSIONSPERCAPITA_MTCO2E", "Usage_MMBTU", "Emissions_MTCO2E", "Number_Accts")

# Initialize an empty list to store summaries
outlier_summaries <- list()

# Summarize outliers for each column and store the results
for(column in columns_to_check) {
  outlier_summaries[[column]] <- summarize_outliers(GIS_EE_encoded, column)
}

# Print summaries for each column
for(column in columns_to_check) {
  summary <- outlier_summaries[[column]]
  cat("Summary for", summary$column, ":\n")
  cat("Number of outliers:", summary$num_outliers, "\n")
  cat("Minimum outlier value:", summary$min_outlier_value, "\n")
  cat("Maximum outlier value:", summary$max_outlier_value, "\n")
  cat("Basic Statistics: \n")
  print(summary$basic_stats)
  cat("\n")
}

```

In the PSE and equity data cleaning process, we prioritized data integrity, ensuring the accuracy and consistency of our datasets throughout their lifecycle. This process began with the crucial step of renaming columns to enhance clarity and consistency, thereby making the datasets more intuitive for analysis.

Type Checking and Format Alignment: We checked data types across all columns, ensuring numeric and categorical data were correctly identified. This step included verifying that numeric columns were accurately typed as integers, and categorical data was appropriately classified. For spatial data integrity, particularly in the conversion back to an sf object, we maintained a strict adherence to ensuring that the coordinate reference system (CRS) was consistently defined, preserving the spatial accuracy essential for geographic analyses.

Data Completeness and Reliability: A comprehensive review for null values was conducted to confirm the completeness of our datasets. By streamlining equity_data_df, we selected only the most pertinent columns related to identifiers, location, and quintile scores across various equity dimensions. The renaming of these columns further contributed to the dataset's clarity.

Standardization for Comparative Analysis: The standardization of quintile labels for the RSE_Quintile column and similar labels across all metrics aligned our datasets with uniform terminology, and comparative analysis across different metrics. This uniformity is especially crucial when merging attributes from equity_data into pse_data, based on matching geographic identifiers.

Ensuring Spatial and Analytical Bounds: Our process included rigorous checks to ensure that all geometries fell within expected geographical boundaries.

Data Merging and Integrity: The inner_join operation, based on matching identifiers between pse_data_df and equity_data_df, was performed to merge relevant attributes. This step was critical in combining datasets without compromising data integrity, avoiding the introduction of duplicate rows or inaccuracies. The conversion of the merged dataframe back into an sf object, ensured the dataset remained conducive to detailed spatial analysis.



# Data Reshaping

Discuss if and why your dataset needed reshaping (GIS did not reshape).

# Data Management

In our project, we've implemented a streamlined collaborative workflow on GitLab, focusing on minimizing collaboration errors through effective data management strategies. Each team member initiates their work in a dedicated branch. This allows for isolated development without impacting the main codebase. Regular commits are made within these branches using `git add .` and `git commit -m "description"`.

Before we start any work in our individual branch, first we update our branch with the main branch, using `git pull origin main`. This streamlines our development process, and also enforces a disciplined data management practice that minimizes errors and fosters efficient collaboration among team members.

# GIS Exploratory Data Analysis

In order to build a comprehensive GIS (Geographic Information System) analysis to visually represent energy usage patterns and emissions across different types of buildings in Seattle, we will use the following attributes below from our final merged data. 

Usage_MMBTU (Gas Consumption): Analyzing gas consumption measured in MMBtu across different buildings can help identify high energy usage patterns. This will help identify which areas or types of buildings consume more energy, potentially indicating areas where energy efficiency measures could be most effective.

Number_Accts (Number of User Accounts): This attribute can provide insights into the density of energy consumers in different areas. When visualized spatially, it can help distinguish between areas with high consumer density versus areas with significant energy usage but fewer accounts, possibly indicating commercial or industrial buildings with high energy demand.

Emissions_MTCO2E (Emissions from Gas Usage): Mapping emissions resulting from gas usage offers a direct visualization of the environmental impact associated with energy consumption in different areas. This analysis is crucial for identifying regions with higher emissions where interventions for cleaner energy use could have a significant environmental benefit.

USAGEPERCAPITA_MMBTU (Gas Usage Per Account): By calculating gas usage per account, we can assess energy efficiency at a more granular level. This metric helps identify buildings or sectors where energy use is disproportionately high relative to the number of accounts, highlighting targets for energy conservation measures or for the transition to cleaner energy sources.

EMISSIONSPERCAPITA_MTCO2E (Emissions Per Account): Similar to USAGEPERCAPITA_MMBTU, analyzing emissions per account provides insights into the carbon footprint of energy consumption on a per-account basis. This can help target specific buildings or sectors for emission reduction initiatives.

```{r Plot Distributions of Key Variables}

# Function to create individual boxplots for a given attribute across years, for each sector
createBoxplotsBySector <- function(data, attribute, title_suffix) {
  ggplot(data, aes(x = as.factor(CAL_YEAR), y = .data[[attribute]])) +
    geom_boxplot(aes(fill = Sector)) +
    facet_wrap(~Sector, scales = "free_y") + 
    theme_minimal() +
    labs(title = paste("Annual", title_suffix, "by Sector"),
         x = "Calendar Year", y = paste(title_suffix, "(MTCO2E)")) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "bottom")
}
createBoxplotsBySector(GIS_EE_encoded, "EMISSIONSPERCAPITA_MTCO2E", "Emissions Per Capita")
createBoxplotsBySector(GIS_EE_encoded, "Usage_MMBTU", "Usage MMBTU")
createBoxplotsBySector(GIS_EE_encoded, "Number_Accts", "Number of Accounts")
createBoxplotsBySector(GIS_EE_encoded, "Emissions_MTCO2E", "Emissions MTCO2E")
createBoxplotsBySector(GIS_EE_encoded, "USAGEPERCAPITA_MMBTU", "Usage Per Capita MMBTU")

```

Above, is a boxplot that focus more closely on the sectors, allowing us to interpret the data for Commercial and Industrial building types more clearly. However, the Industrial sector still presents challenges in interpretation, aside from the evident potential outliers in some industrial buildings.

```{r Key Metrics/Measurements/Statistics}

# Identify all unique sectors in the dataset
unique_sectors <- unique(GIS_EE_encoded$Sector)

# Iterate over each sector to filter data and print summary statistics
for(sector in unique_sectors) {
  cat("Summary statistics for sector:", sector, "\n")
  
  # Filter the dataset for the current sector
  sector_data <- GIS_EE_encoded %>% 
    filter(Sector == sector)
  
  # Get a summary to see the range and quartiles for EMISSIONSPERCAPITA_MTCO2E
  summary_stats <- summary(sector_data$EMISSIONSPERCAPITA_MTCO2E)
  
  print(summary_stats)
  cat("\n")
}

```


```{r Analyzing Data Over Time}

# Function to create a ggplot for the trend of a given attribute over years
plotTrendOverYears <- function(data, attribute, title) {
  ggplot(data, aes_string(x = "CAL_YEAR", y = attribute, group = "1")) +
    geom_line() +
    geom_point() +
    theme_minimal() +
    labs(title = title, x = "Calendar Year", y = paste("Value (", attribute, ")"))
}

plotTrendOverYears(GIS_EE_encoded, "Usage_MMBTU", "Trend of Usage MMBTU Over Years")
plotTrendOverYears(GIS_EE_encoded, "Number_Accts", "Trend of Number of Accounts Over Years")
plotTrendOverYears(GIS_EE_encoded, "Emissions_MTCO2E", "Trend of Emissions MTCO2E Over Years")
plotTrendOverYears(GIS_EE_encoded, "USAGEPERCAPITA_MMBTU", "Trend of Usage Per Capita MMBTU Over Years")
plotTrendOverYears(GIS_EE_encoded, "EMISSIONSPERCAPITA_MTCO2E", "Trend of Emissions Per Capita MTCO2E Over Years")


```


We explore energy usage and emissions from 2020 to 2023 and compare it among various sectors (building types). The first graph is a line plot that allows us to observe trends over the years. We can see that the trend of emissions per capita is declining, which is a positive outcome. 


```{r Comparison Across Categories}

# Function to create a ggplot for yearly trends of a given attribute, grouped by sector
createYearlyTrendPlot <- function(data, attribute, title_prefix) {
  ggplot(data) +
    geom_line(aes_string(x = "CAL_YEAR", y = attribute, color = "Sector"), size = 1) +
    geom_point(aes_string(x = "CAL_YEAR", y = attribute, color = "Sector")) +
    facet_wrap(~Sector, scales = "free_y") +
    theme_minimal() +
    labs(title = paste(title_prefix, "Yearly Trend by Sector"),
         x = "Calendar Year",
         y = paste(attribute, "(MTCO2E)")) +
    theme(legend.position = "none")
}
createYearlyTrendPlot(GIS_EE_encoded, "EMISSIONSPERCAPITA_MTCO2E", "Emissions Per Capita")
createYearlyTrendPlot(GIS_EE_encoded, "Usage_MMBTU", "Usage MMBTU")
createYearlyTrendPlot(GIS_EE_encoded, "Number_Accts", "Number of Accounts")
createYearlyTrendPlot(GIS_EE_encoded, "Emissions_MTCO2E", "Emissions MTCO2E")
createYearlyTrendPlot(GIS_EE_encoded, "USAGEPERCAPITA_MMBTU", "Usage Per Capita MMBTU")

```

Above, we employ a facet wrap to visualize the yearly trend for each sector. These visualizations will help us understand how energy usage and emissions have changed over time and how these changes vary across different building sectors.

```{r Pattern Identification and Interpretation, leaflet-map, fig.cap="A Leaflet Map of Emissions", echo=FALSE}

# Function to create an interactive map for a given data column
createInteractiveMap <- function(data, column_name, colorPalette = "viridis", title = "") {
  # Determine the color palette
  palette <- colorNumeric(palette = colorPalette, domain = data[[column_name]])
  
  # Create the map
  leaflet(data) %>%
    addTiles() %>%
    addPolygons(color = ~palette(data[[column_name]]),
                fillOpacity = 0.5,
                popup = ~paste(title, ":", data[[column_name]])) %>%
    addLegend("bottomright", pal = palette, values = data[[column_name]],
              title = title,
              labFormat = labelFormat(),
              opacity = 1)
}
createInteractiveMap(GIS_EE_encoded, "EMISSIONSPERCAPITA_MTCO2E", "viridis", "Emissions Per Capita MTCO2E")
createInteractiveMap(GIS_EE_encoded, "Usage_MMBTU", "viridis", "Usage MMBTU")
createInteractiveMap(GIS_EE_encoded, "Number_Accts", "viridis", "Number of Accounts")
createInteractiveMap(GIS_EE_encoded, "Emissions_MTCO2E", "viridis", "Emissions MTCO2E")
createInteractiveMap(GIS_EE_encoded, "USAGEPERCAPITA_MMBTU", "viridis", "Usage Per Capita MMBTU")

```


We aimed to generate geographic plots to visualize the distribution of our data points across Seattle. It appears that some energy data was omitted during the inner join. Below, we will graph the energy data before it was merged with the equity data. This omission could be due to mismatches between GEOID and Census Tract identifiers, raising questions about the extent of additional data available when examining only the energy data.


```{r Analysis of Outliers/Anomalies, leaflet-map, fig.cap="A Leaflet Map of Emissions", echo=FALSE}

# EMISSIONSPERCAPITA_MTCO2E
leaflet(pse_data_clean) %>%
  addTiles() %>%
  addPolygons(color = ~colorNumeric("viridis", EMISSIONSP)(EMISSIONSP),
              fillOpacity = 0.5,
              popup = ~paste("Emissions Per Capita MTCO2E:", EMISSIONSP))

# Usage_MMBTU
leaflet(pse_data_clean) %>%
  addTiles() %>%
  addPolygons(color = ~colorNumeric("viridis", Usage_MMBT)(Usage_MMBT),
              fillOpacity = 0.5,
              popup = ~paste("Usage MMBTU:", Usage_MMBT))

# Number_Accts
leaflet(pse_data_clean) %>%
  addTiles() %>%
  addPolygons(color = ~colorNumeric("viridis", Number_Acc)(Number_Acc),
              fillOpacity = 0.5,
              popup = ~paste("Number of Accounts:", Number_Acc))

# Emissions_MTCO2E
leaflet(pse_data_clean) %>%
  addTiles() %>%
  addPolygons(color = ~colorNumeric("viridis", Emissions_)(Emissions_),
              fillOpacity = 0.5,
              popup = ~paste("Emissions MTCO2E:", Emissions_))

# USAGEPERCAPITA_MMBTU
leaflet(pse_data_clean) %>%
  addTiles() %>%
  addPolygons(color = ~colorNumeric("viridis", USAGEPERCA)(USAGEPERCA),
              fillOpacity = 0.5,
              popup = ~paste("Usage Per Capita MMBTU:", USAGEPERCA))

```


The geographic plot, utilizing energy data prior to merging with the equity data, reveals an increased number of data points. We decided on the best path forward. We will divide the analysis into two distinct sections for GIS: one focusing on examining energy data through an equity lens, and the other concentrating on analyzing energy usage across the Seattle area.

# Building Performance Data EDA

```{r}
#make distribution graphs for Total Gross Floor Area and median year built
library(ggplot2)
ggplot(combined_data, aes(x = Total_GFA)) +
  geom_histogram(fill = "blue") +
  labs(title = "Distribution of Total Gross Floor Area",
       x = "Total Gross Floor Area",
       y = "Frequency")

ggplot(combined_data, aes(x = Median_Year_Built)) +
  geom_histogram(fill = "blue") +
  labs(title = "Distribution of Median Year Built",
       x = "Median Year Built",
       y = "Frequency")
```


```{r}
#get the summary data and t tests for gross floor area and median es score
print("Total Gross Floor Area")
summary(combined_data$Total_GFA)
gfa.p.val <- t.test(combined_data$Total_GFA)
print(gfa.p.val)  
  
print("Median ES Score")
summary(combined_data$Median_ES_Score)
median_es.p.val <- t.test(combined_data$Median_ES_Score)
print(median_es.p.val)  
```


```{r}
ggplot(combined_data, aes(x = Median_Year_Built, y = Total_GFA)) +
  geom_point() +
  labs(title = "Scatterplot between Median Year Built and Total_GFA",
       x = "Median Year Built",
       y = "Total_GFA")
```


```{r}
#plot median year built vs median es score
ggplot(combined_data, aes(x = Median_Year_Built, y = Median_EUI)) +
  geom_point() +
  labs(title = "Median_EUI and Median_Year_Built",
       x = "Median Year Built",
       y = "Median_EUI")
```

```{r}
#plot the gross floor area over time
ggplot(combined_data, aes(x = as.factor(Year), y = Median_EUI)) +
  geom_line() +
  labs(title = "Total Gross Floor Area Over Time",
       x = "Year",
       y = "Total Gross Floor Area")
```

```{r}
#plot the total gross floor area by vuilding type
ggplot(combined_data, aes(x = BuildingType, y = Total_GFA)) +
  geom_boxplot() +
  labs(title = "Total Gross Floor Area Across Building Types",
       x = "Building Type",
       y = "Total Gross Floor Area") +
  theme(axis.text.x = element_text(angle = 90, size = 5)) 
```

# Conclusion

Summarize the key findings and outline any recommendations.

# References

List all data sources and other references used in your analysis.
